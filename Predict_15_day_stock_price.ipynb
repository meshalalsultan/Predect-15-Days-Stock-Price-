{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict 15 day stock price.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LWh2MkhQO3O",
        "outputId": "7ced0496-79fb-4e99-b1dd-4d0d1244efb7"
      },
      "source": [
        "!pip install investpy\n",
        "!pip install yahoo_fin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: investpy in /usr/local/lib/python3.6/dist-packages (1.0)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from investpy) (4.6.2)\n",
            "Requirement already satisfied: setuptools>=41.2.0 in /usr/local/lib/python3.6/dist-packages (from investpy) (51.1.2)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.6/dist-packages (from investpy) (1.1.5)\n",
            "Requirement already satisfied: Unidecode>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from investpy) (1.1.2)\n",
            "Requirement already satisfied: pytz>=2019.3 in /usr/local/lib/python3.6/dist-packages (from investpy) (2020.5)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.6/dist-packages (from investpy) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.6/dist-packages (from investpy) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.1->investpy) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->investpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->investpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->investpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->investpy) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.1->investpy) (1.15.0)\n",
            "Requirement already satisfied: yahoo_fin in /usr/local/lib/python3.6/dist-packages (0.8.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aApzOh4aQFRV",
        "outputId": "f85f40cb-e82f-4a92-afba-cf3957a91a3a"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import deque\n",
        "from yahoo_fin import stock_info as si\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import investpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning - Certain functionality \n",
            "             requires requests_html, which is not installed.\n",
            "             \n",
            "             Install using: \n",
            "             pip install requests_html\n",
            "             \n",
            "             After installation, you may have to restart your Python session.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkvlBvWpQRUK"
      },
      "source": [
        "# set seed, so we can get the same results after rerunning several times\n",
        "np.random.seed(314)\n",
        "tf.random.set_seed(314)\n",
        "random.seed(314)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbMc1GSIQcBV"
      },
      "source": [
        "def shuffle_in_unison(a, b):\n",
        "    # shuffle two arrays in the same way\n",
        "    state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(state)\n",
        "    np.random.shuffle(b)\n",
        "\n",
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
        "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "    \"\"\"\n",
        "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
        "    Params:\n",
        "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
        "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
        "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
        "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
        "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
        "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
        "            to False will split datasets in a random way\n",
        "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
        "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
        "    \"\"\"\n",
        "    # see if ticker is already a loaded stock from yahoo finance\n",
        "    if isinstance(ticker, str):\n",
        "        # load it from yahoo_fin library\n",
        "        df = si.get_data(ticker)\n",
        "    elif isinstance(ticker, pd.DataFrame):\n",
        "        # already loaded, use it directly\n",
        "        df = ticker\n",
        "    else:\n",
        "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
        "    # add date as a column\n",
        "    if \"date\" not in df.columns:\n",
        "        df[\"date\"] = df.index\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
        "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
        "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
        "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    if split_by_date:\n",
        "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
        "        train_samples = int((1 - test_size) * len(X))\n",
        "        result[\"X_train\"] = X[:train_samples]\n",
        "        result[\"y_train\"] = y[:train_samples]\n",
        "        result[\"X_test\"]  = X[train_samples:]\n",
        "        result[\"y_test\"]  = y[train_samples:]\n",
        "        if shuffle:\n",
        "            # shuffle the datasets for training (if shuffle parameter is set)\n",
        "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
        "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
        "    else:    \n",
        "        # split the dataset randomly\n",
        "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
        "                                                                                test_size=test_size, shuffle=shuffle)\n",
        "    # get the list of test set dates\n",
        "    dates = result[\"X_test\"][:, -1, -1]\n",
        "    # retrieve test features from the original dataframe\n",
        "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
        "    # remove duplicated dates in the testing dataframe\n",
        "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
        "    # remove dates from the training/testing sets & convert to float32\n",
        "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    return result"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25-bvaIsRU00"
      },
      "source": [
        "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVbccaBDRcfV"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# Window size or the sequence length\n",
        "N_STEPS = 50\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 15\n",
        "# whether to scale feature columns & output price as well\n",
        "SCALE = True\n",
        "scale_str = f\"sc-{int(SCALE)}\"\n",
        "# whether to shuffle the dataset\n",
        "SHUFFLE = True\n",
        "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
        "# whether to split the training/testing set by date\n",
        "SPLIT_BY_DATE = False\n",
        "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "# features to use\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "# date now\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "### model parameters\n",
        "N_LAYERS = 2\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 256\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "# whether to use bidirectional RNNs\n",
        "BIDIRECTIONAL = False\n",
        "### training parameters\n",
        "# mean absolute error loss\n",
        "# LOSS = \"mae\"\n",
        "# huber loss\n",
        "LOSS = \"huber_loss\"\n",
        "OPTIMIZER = \"adam\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "# Amazon stock market\n",
        "ticker = \"GM\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "# model name to save, making it as unique as possible based on parameters\n",
        "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
        "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
        "if BIDIRECTIONAL:\n",
        "    model_name += \"-b\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rQ8hpbwRhsZ"
      },
      "source": [
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw5OgYcTRlVQ",
        "outputId": "f7ad23e3-682f-4211-f7a9-000c2ddecffd"
      },
      "source": [
        "# load the data\n",
        "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
        "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
        "                feature_columns=FEATURE_COLUMNS)\n",
        "# save the dataframe\n",
        "data[\"df\"].to_csv(ticker_data_filename)\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
        "# some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "# train the model and save the weights whenever we see \n",
        "# a new optimal model using ModelCheckpoint\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "32/32 [==============================] - 6s 46ms/step - loss: 0.0135 - mean_absolute_error: 0.1137 - val_loss: 0.0018 - val_mean_absolute_error: 0.0423\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00180, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0024 - mean_absolute_error: 0.0508 - val_loss: 0.0018 - val_mean_absolute_error: 0.0420\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00180\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0022 - mean_absolute_error: 0.0488 - val_loss: 0.0026 - val_mean_absolute_error: 0.0535\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00180\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0022 - mean_absolute_error: 0.0496 - val_loss: 0.0017 - val_mean_absolute_error: 0.0397\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00180 to 0.00166, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0021 - mean_absolute_error: 0.0467 - val_loss: 0.0019 - val_mean_absolute_error: 0.0436\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00166\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0022 - mean_absolute_error: 0.0483 - val_loss: 0.0021 - val_mean_absolute_error: 0.0454\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00166\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0023 - mean_absolute_error: 0.0492 - val_loss: 0.0021 - val_mean_absolute_error: 0.0460\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00166\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0021 - mean_absolute_error: 0.0489 - val_loss: 0.0019 - val_mean_absolute_error: 0.0427\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00166\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0022 - mean_absolute_error: 0.0497 - val_loss: 0.0021 - val_mean_absolute_error: 0.0455\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00166\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0023 - mean_absolute_error: 0.0490 - val_loss: 0.0020 - val_mean_absolute_error: 0.0454\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00166\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0022 - mean_absolute_error: 0.0478 - val_loss: 0.0017 - val_mean_absolute_error: 0.0401\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00166\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0021 - mean_absolute_error: 0.0469 - val_loss: 0.0017 - val_mean_absolute_error: 0.0411\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00166\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0020 - mean_absolute_error: 0.0458 - val_loss: 0.0016 - val_mean_absolute_error: 0.0395\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00166 to 0.00162, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0019 - mean_absolute_error: 0.0448 - val_loss: 0.0016 - val_mean_absolute_error: 0.0390\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00162\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0023 - mean_absolute_error: 0.0497 - val_loss: 0.0016 - val_mean_absolute_error: 0.0396\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00162\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0020 - mean_absolute_error: 0.0463 - val_loss: 0.0020 - val_mean_absolute_error: 0.0454\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00162\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0022 - mean_absolute_error: 0.0501 - val_loss: 0.0021 - val_mean_absolute_error: 0.0483\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00162\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0021 - mean_absolute_error: 0.0482 - val_loss: 0.0016 - val_mean_absolute_error: 0.0400\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00162\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0020 - mean_absolute_error: 0.0471 - val_loss: 0.0018 - val_mean_absolute_error: 0.0415\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00162\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0459 - val_loss: 0.0021 - val_mean_absolute_error: 0.0465\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00162\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0024 - mean_absolute_error: 0.0510 - val_loss: 0.0016 - val_mean_absolute_error: 0.0385\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.00162 to 0.00157, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0020 - mean_absolute_error: 0.0449 - val_loss: 0.0019 - val_mean_absolute_error: 0.0434\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00157\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0021 - mean_absolute_error: 0.0479 - val_loss: 0.0019 - val_mean_absolute_error: 0.0427\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00157\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0443 - val_loss: 0.0016 - val_mean_absolute_error: 0.0385\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00157 to 0.00156, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0452 - val_loss: 0.0016 - val_mean_absolute_error: 0.0397\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00156\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0020 - mean_absolute_error: 0.0454 - val_loss: 0.0018 - val_mean_absolute_error: 0.0421\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00156\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0431 - val_loss: 0.0016 - val_mean_absolute_error: 0.0390\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00156\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0020 - mean_absolute_error: 0.0448 - val_loss: 0.0016 - val_mean_absolute_error: 0.0384\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00156 to 0.00156, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0439 - val_loss: 0.0017 - val_mean_absolute_error: 0.0395\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00156\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0435 - val_loss: 0.0017 - val_mean_absolute_error: 0.0407\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00156\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.0021 - mean_absolute_error: 0.0471 - val_loss: 0.0016 - val_mean_absolute_error: 0.0383\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00156\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0430 - val_loss: 0.0016 - val_mean_absolute_error: 0.0393\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00156\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0438 - val_loss: 0.0016 - val_mean_absolute_error: 0.0387\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00156\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0424 - val_loss: 0.0015 - val_mean_absolute_error: 0.0382\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00156 to 0.00154, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0423 - val_loss: 0.0017 - val_mean_absolute_error: 0.0405\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00154\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0020 - mean_absolute_error: 0.0448 - val_loss: 0.0016 - val_mean_absolute_error: 0.0393\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00154\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0021 - mean_absolute_error: 0.0453 - val_loss: 0.0016 - val_mean_absolute_error: 0.0385\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00154\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0443 - val_loss: 0.0015 - val_mean_absolute_error: 0.0381\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00154\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0018 - mean_absolute_error: 0.0434 - val_loss: 0.0016 - val_mean_absolute_error: 0.0395\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00154\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0442 - val_loss: 0.0016 - val_mean_absolute_error: 0.0389\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00154\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0449 - val_loss: 0.0015 - val_mean_absolute_error: 0.0381\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00154\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0460 - val_loss: 0.0017 - val_mean_absolute_error: 0.0392\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00154\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0020 - mean_absolute_error: 0.0460 - val_loss: 0.0016 - val_mean_absolute_error: 0.0386\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00154\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0425 - val_loss: 0.0015 - val_mean_absolute_error: 0.0378\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00154 to 0.00151, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0446 - val_loss: 0.0015 - val_mean_absolute_error: 0.0380\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00151\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0432 - val_loss: 0.0018 - val_mean_absolute_error: 0.0427\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00151\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0431 - val_loss: 0.0016 - val_mean_absolute_error: 0.0392\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00151\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0434 - val_loss: 0.0015 - val_mean_absolute_error: 0.0385\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00151\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0439 - val_loss: 0.0015 - val_mean_absolute_error: 0.0379\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00151\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0432 - val_loss: 0.0016 - val_mean_absolute_error: 0.0393\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00151\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0439 - val_loss: 0.0015 - val_mean_absolute_error: 0.0378\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00151 to 0.00151, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0420 - val_loss: 0.0016 - val_mean_absolute_error: 0.0388\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00151\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0442 - val_loss: 0.0016 - val_mean_absolute_error: 0.0405\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00151\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0438 - val_loss: 0.0015 - val_mean_absolute_error: 0.0379\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00151\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0431 - val_loss: 0.0015 - val_mean_absolute_error: 0.0380\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00151 to 0.00151, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0427 - val_loss: 0.0015 - val_mean_absolute_error: 0.0379\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00151\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0440 - val_loss: 0.0015 - val_mean_absolute_error: 0.0379\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00151\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0429 - val_loss: 0.0016 - val_mean_absolute_error: 0.0394\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00151\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0436 - val_loss: 0.0016 - val_mean_absolute_error: 0.0395\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00151\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0445 - val_loss: 0.0015 - val_mean_absolute_error: 0.0377\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00151 to 0.00150, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0432 - val_loss: 0.0015 - val_mean_absolute_error: 0.0385\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00150\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0424 - val_loss: 0.0016 - val_mean_absolute_error: 0.0385\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00150\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0442 - val_loss: 0.0015 - val_mean_absolute_error: 0.0378\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00150\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0425 - val_loss: 0.0015 - val_mean_absolute_error: 0.0379\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00150\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0437 - val_loss: 0.0015 - val_mean_absolute_error: 0.0376\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00150 to 0.00150, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0440 - val_loss: 0.0017 - val_mean_absolute_error: 0.0421\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00150\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0432 - val_loss: 0.0020 - val_mean_absolute_error: 0.0451\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00150\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0024 - mean_absolute_error: 0.0490 - val_loss: 0.0017 - val_mean_absolute_error: 0.0410\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00150\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0441 - val_loss: 0.0018 - val_mean_absolute_error: 0.0416\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00150\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0430 - val_loss: 0.0015 - val_mean_absolute_error: 0.0375\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00150\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0434 - val_loss: 0.0018 - val_mean_absolute_error: 0.0426\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00150\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0433 - val_loss: 0.0015 - val_mean_absolute_error: 0.0379\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00150\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0016 - mean_absolute_error: 0.0421 - val_loss: 0.0015 - val_mean_absolute_error: 0.0382\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00150\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0428 - val_loss: 0.0015 - val_mean_absolute_error: 0.0380\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00150\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0016 - mean_absolute_error: 0.0411 - val_loss: 0.0015 - val_mean_absolute_error: 0.0375\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00150\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0437 - val_loss: 0.0015 - val_mean_absolute_error: 0.0385\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00150\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0433 - val_loss: 0.0017 - val_mean_absolute_error: 0.0413\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00150\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0430 - val_loss: 0.0018 - val_mean_absolute_error: 0.0410\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00150\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0441 - val_loss: 0.0015 - val_mean_absolute_error: 0.0378\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00150\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0417 - val_loss: 0.0016 - val_mean_absolute_error: 0.0398\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00150\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0438 - val_loss: 0.0016 - val_mean_absolute_error: 0.0398\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00150\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0438 - val_loss: 0.0016 - val_mean_absolute_error: 0.0390\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00150\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.0019 - mean_absolute_error: 0.0447 - val_loss: 0.0015 - val_mean_absolute_error: 0.0376\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00150\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0019 - mean_absolute_error: 0.0432 - val_loss: 0.0015 - val_mean_absolute_error: 0.0379\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00150\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0016 - mean_absolute_error: 0.0417 - val_loss: 0.0016 - val_mean_absolute_error: 0.0396\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00150\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0424 - val_loss: 0.0016 - val_mean_absolute_error: 0.0395\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00150\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0434 - val_loss: 0.0017 - val_mean_absolute_error: 0.0417\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00150\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0020 - mean_absolute_error: 0.0454 - val_loss: 0.0016 - val_mean_absolute_error: 0.0382\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00150\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0016 - mean_absolute_error: 0.0412 - val_loss: 0.0020 - val_mean_absolute_error: 0.0461\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00150\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0432 - val_loss: 0.0015 - val_mean_absolute_error: 0.0382\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00150\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0016 - mean_absolute_error: 0.0412 - val_loss: 0.0015 - val_mean_absolute_error: 0.0377\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.00150 to 0.00150, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0422 - val_loss: 0.0015 - val_mean_absolute_error: 0.0378\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.00150 to 0.00149, saving model to results/2021-01-20_GM-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0016 - mean_absolute_error: 0.0408 - val_loss: 0.0015 - val_mean_absolute_error: 0.0377\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00149\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0422 - val_loss: 0.0016 - val_mean_absolute_error: 0.0396\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00149\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0418 - val_loss: 0.0015 - val_mean_absolute_error: 0.0381\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00149\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0016 - mean_absolute_error: 0.0413 - val_loss: 0.0015 - val_mean_absolute_error: 0.0378\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00149\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0432 - val_loss: 0.0015 - val_mean_absolute_error: 0.0377\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00149\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018 - mean_absolute_error: 0.0444 - val_loss: 0.0016 - val_mean_absolute_error: 0.0395\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00149\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0016 - mean_absolute_error: 0.0417 - val_loss: 0.0015 - val_mean_absolute_error: 0.0382\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00149\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0017 - mean_absolute_error: 0.0429 - val_loss: 0.0016 - val_mean_absolute_error: 0.0392\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJObz_aBRnHR"
      },
      "source": [
        "#tensorboard --logdir=\"logs\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlZ8T6L2R6Zt"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graph(test_df):\n",
        "    \"\"\"\n",
        "    This function plots true close price along with predicted close price\n",
        "    with blue and red colors respectively\n",
        "    \"\"\"\n",
        "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
        "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
        "    plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxUCUTd2R8_i"
      },
      "source": [
        "def get_final_df(model, data):\n",
        "    \"\"\"\n",
        "    This function takes the `model` and `data` dict to \n",
        "    construct a final dataframe that includes the features along \n",
        "    with true and predicted prices of the testing dataset\n",
        "    \"\"\"\n",
        "    # if predicted future price is higher than the current, \n",
        "    # then calculate the true future price minus the current price, to get the buy profit\n",
        "    buy_profit  = lambda current, true_future, pred_future: true_future - current if pred_future > current else 0\n",
        "    # if the predicted future price is lower than the current price,\n",
        "    # then subtract the true future price from the current price\n",
        "    sell_profit = lambda current, true_future, pred_future: current - true_future if pred_future < current else 0\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_test = data[\"y_test\"]\n",
        "    # perform prediction and get prices\n",
        "    y_pred = model.predict(X_test)\n",
        "    if SCALE:\n",
        "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    test_df = data[\"test_df\"]\n",
        "    # add predicted future prices to the dataframe\n",
        "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
        "    # add true future prices to the dataframe\n",
        "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
        "    # sort the dataframe by date\n",
        "    test_df.sort_index(inplace=True)\n",
        "    final_df = test_df\n",
        "    # add the buy profit column\n",
        "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
        "                                    final_df[\"adjclose\"], \n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    # add the sell profit column\n",
        "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
        "                                    final_df[\"adjclose\"], \n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    return final_df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqGTQ5lgR_on"
      },
      "source": [
        "def predict(model, data):\n",
        "    # retrieve the last sequence from data\n",
        "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
        "    # expand dimension\n",
        "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "    # get the prediction (scaled from 0 to 1)\n",
        "    prediction = model.predict(last_sequence)\n",
        "    # get the price (by inverting the scaling)\n",
        "    if SCALE:\n",
        "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
        "    else:\n",
        "        predicted_price = prediction[0][0]\n",
        "    return predicted_price"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RXpDUePSBe5"
      },
      "source": [
        "# load optimal model weights from results folder\n",
        "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
        "model.load_weights(model_path)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFMUzINJSDVM"
      },
      "source": [
        "# evaluate the model\n",
        "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
        "# calculate the mean absolute error (inverse scaling)\n",
        "if SCALE:\n",
        "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
        "else:\n",
        "    mean_absolute_error = mae"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W-qnyIRSFPD"
      },
      "source": [
        "# get the final dataframe for the testing set\n",
        "final_df = get_final_df(model, data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0ovWBRSSG31"
      },
      "source": [
        "# predict the future price\n",
        "future_price = predict(model, data)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPBlX1kfSKqY"
      },
      "source": [
        "# we calculate the accuracy by counting the number of positive profits\n",
        "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
        "# calculating total buy & sell profit\n",
        "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
        "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
        "# total profit by adding sell & buy together\n",
        "total_profit = total_buy_profit + total_sell_profit\n",
        "# dividing total profit by number of testing samples (number of trades)\n",
        "profit_per_trade = total_profit / len(final_df)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFTHoPVJSMsQ",
        "outputId": "ba773ed2-1181-490b-d643-19f9f407e962"
      },
      "source": [
        "# printing metrics\n",
        "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
        "print(f\"{LOSS} loss:\", loss)\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
        "print(\"Accuracy score:\", accuracy_score)\n",
        "print(\"Total buy profit:\", total_buy_profit)\n",
        "print(\"Total sell profit:\", total_sell_profit)\n",
        "print(\"Total profit:\", total_profit)\n",
        "print(\"Profit per trade:\", profit_per_trade)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Future price after 15 days is 49.30$\n",
            "huber_loss loss: 0.001492248964495957\n",
            "Mean Absolute Error: 15.947604259230143\n",
            "Accuracy score: 0.5170340681362725\n",
            "Total buy profit: -29.353721618652344\n",
            "Total sell profit: 69.99937915802002\n",
            "Total profit: 40.645657539367676\n",
            "Profit per trade: 0.08145422352578692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "P4uhruuMSO5R",
        "outputId": "e6cd25e2-51b4-45c8-d500-96e6f57370f8"
      },
      "source": [
        "# plot true/pred prices graph\n",
        "plot_graph(final_df)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8bpinjsSQ96",
        "outputId": "7295f0d3-f0af-4bd2-84f0-499d9b6235d3"
      },
      "source": [
        "print(final_df.tail(10))\n",
        "# save the final dataframe to csv-results folder\n",
        "csv_results_folder = \"csv-results\"\n",
        "if not os.path.isdir(csv_results_folder):\n",
        "    os.mkdir(csv_results_folder)\n",
        "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
        "final_df.to_csv(csv_filename)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 open       high  ...  buy_profit  sell_profit\n",
            "2020-08-04  25.870001  26.030001  ...    1.111582     0.000000\n",
            "2020-08-31  30.020000  30.190001  ...    0.000000    -1.243261\n",
            "2020-09-03  30.930000  31.520000  ...    0.000000    -1.867264\n",
            "2020-09-10  31.820000  31.879999  ...    2.164160     0.000000\n",
            "2020-09-23  29.490000  29.990000  ...    1.415997     0.000000\n",
            "2020-10-05  30.850000  31.290001  ...    0.806856     0.000000\n",
            "2020-10-29  34.509998  35.660000  ...    0.081547     0.000000\n",
            "2020-11-03  35.099998  35.990002  ...   -0.147785     0.000000\n",
            "2020-11-20  42.810001  43.320000  ...    0.000000     1.541412\n",
            "2020-12-22  41.349998  41.400002  ...   -0.848820     0.000000\n",
            "\n",
            "[10 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS-UoPU3STlT"
      },
      "source": [
        "The Artical in : https://www.thepythoncode.com/article/stock-price-prediction-in-python-using-tensorflow-2-and-keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "BSThXJwbZUlX",
        "outputId": "e5f37c26-0d5d-45ef-8463-3baa27713c54"
      },
      "source": [
        "final_df.tail()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>adjclose</th>\n",
              "      <th>volume</th>\n",
              "      <th>ticker</th>\n",
              "      <th>adjclose_15</th>\n",
              "      <th>true_adjclose_15</th>\n",
              "      <th>buy_profit</th>\n",
              "      <th>sell_profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-10-05</th>\n",
              "      <td>30.850000</td>\n",
              "      <td>31.290001</td>\n",
              "      <td>30.799999</td>\n",
              "      <td>30.959999</td>\n",
              "      <td>30.959999</td>\n",
              "      <td>8314100</td>\n",
              "      <td>GM</td>\n",
              "      <td>31.766855</td>\n",
              "      <td>35.820000</td>\n",
              "      <td>0.806856</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-10-29</th>\n",
              "      <td>34.509998</td>\n",
              "      <td>35.660000</td>\n",
              "      <td>34.360001</td>\n",
              "      <td>34.889999</td>\n",
              "      <td>34.889999</td>\n",
              "      <td>14230400</td>\n",
              "      <td>GM</td>\n",
              "      <td>34.971546</td>\n",
              "      <td>42.820000</td>\n",
              "      <td>0.081547</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-11-03</th>\n",
              "      <td>35.099998</td>\n",
              "      <td>35.990002</td>\n",
              "      <td>34.889999</td>\n",
              "      <td>35.349998</td>\n",
              "      <td>35.349998</td>\n",
              "      <td>11439100</td>\n",
              "      <td>GM</td>\n",
              "      <td>35.202213</td>\n",
              "      <td>46.459999</td>\n",
              "      <td>-0.147785</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-11-20</th>\n",
              "      <td>42.810001</td>\n",
              "      <td>43.320000</td>\n",
              "      <td>42.369999</td>\n",
              "      <td>43.040001</td>\n",
              "      <td>43.040001</td>\n",
              "      <td>16881200</td>\n",
              "      <td>GM</td>\n",
              "      <td>41.498589</td>\n",
              "      <td>41.619999</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.541412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-22</th>\n",
              "      <td>41.349998</td>\n",
              "      <td>41.400002</td>\n",
              "      <td>40.549999</td>\n",
              "      <td>40.900002</td>\n",
              "      <td>40.900002</td>\n",
              "      <td>10559000</td>\n",
              "      <td>GM</td>\n",
              "      <td>40.051182</td>\n",
              "      <td>51.529999</td>\n",
              "      <td>-0.848820</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 open       high  ...  buy_profit  sell_profit\n",
              "2020-10-05  30.850000  31.290001  ...    0.806856     0.000000\n",
              "2020-10-29  34.509998  35.660000  ...    0.081547     0.000000\n",
              "2020-11-03  35.099998  35.990002  ...   -0.147785     0.000000\n",
              "2020-11-20  42.810001  43.320000  ...    0.000000     1.541412\n",
              "2020-12-22  41.349998  41.400002  ...   -0.848820     0.000000\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbP-WlwwZ0Lm"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}